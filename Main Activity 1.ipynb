{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Activity 1: Latent Semantic Analysis with SVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datafile = 'abcnews-date-text.csv'\n",
    "raw_data = pd.read_csv(datafile, parse_dates=[0], infer_datetime_format=True)\n",
    "\n",
    "reindexed_data = raw_data['headline_text']\n",
    "reindexed_data.index = raw_data['publish_date']\n",
    "\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data exploration 1:  Plot word frequency to see the core vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def get_top_n_words_1(n_top_words, count_vectorizer, text_data):\n",
    "    '''\n",
    "    returns a tuple of the top n words in a sample and their \n",
    "    accompanying counts, given a CountVectorizer object and text sample\n",
    "    '''\n",
    "    vectorized_headlines = count_vectorizer.fit_transform(text_data.values)\n",
    "    vectorized_total = np.sum(vectorized_headlines, axis=0)\n",
    "    word_indices = np.flip(np.argsort(vectorized_total)[0,:], 1)\n",
    "    word_values = np.flip(np.sort(vectorized_total)[0,:],1)\n",
    "    \n",
    "    word_vectors = np.zeros((n_top_words, vectorized_headlines.shape[1]))\n",
    "    for i in range(n_top_words):\n",
    "        word_vectors[i,word_indices[0,i]] = 1\n",
    "\n",
    "    words = [word[0].encode('ascii').decode('utf-8') for \n",
    "             word in count_vectorizer.inverse_transform(word_vectors)]\n",
    "\n",
    "    return (words, word_values[0,:n_top_words].tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "words, word_values = get_top_n_words_1(n_top_words=15,\n",
    "                                     count_vectorizer=count_vectorizer, \n",
    "                                     text_data=reindexed_data)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.bar(range(len(words)), word_values);\n",
    "ax.set_xticks(range(len(words)));\n",
    "ax.set_xticklabels(words, rotation='vertical');\n",
    "ax.set_title('Top words in headlines dataset (excluding stop words)');\n",
    "ax.set_xlabel('Word');\n",
    "ax.set_ylabel('Number of occurences');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data exploration 2:  Plot time series of news counts to get a sense of the sample density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_counts = reindexed_data.resample('M').count()\n",
    "yearly_counts = reindexed_data.resample('A').count()\n",
    "daily_counts = reindexed_data.resample('D').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(3, figsize=(18,16))\n",
    "ax[0].plot(daily_counts);\n",
    "ax[0].set_title('Daily Counts');\n",
    "ax[1].plot(monthly_counts);\n",
    "ax[1].set_title('Monthly Counts');\n",
    "ax[2].plot(yearly_counts);\n",
    "ax[2].set_title('Yearly Counts');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Using the full data dataset\n",
    "small_count_vectorizer = CountVectorizer(stop_words='english', max_features=40000)\n",
    "print('Headline before vectorization: {}'.format(reindexed_data[123]))\n",
    "small_document_term_matrix = small_count_vectorizer.fit_transform(reindexed_data)\n",
    "print('Headline after vectorization: \\n{}'.format(small_document_term_matrix[123]))\n",
    "\n",
    "#If your machine takes a long time to run the code above (usually around 10 min), you can use the following four lines of code instead.\n",
    "\n",
    "#small_text_sample = reindexed_data.sample(n=10000, random_state=0).values\n",
    "#print('Headline before vectorization: {}'.format(small_text_sample[123]))\n",
    "#small_document_term_matrix = small_count_vectorizer.fit_transform(small_text_sample)\n",
    "#print('Headline after vectorization: \\n{}'.format(small_document_term_matrix[123]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defind number of topic\n",
    "n_topics = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calling the LSA model which is effectively just a truncated singular value decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_model = TruncatedSVD(n_components=n_topics)\n",
    "lsa_topic_matrix = lsa_model.fit_transform(small_document_term_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def get_keys(topic_matrix):\n",
    "    '''\n",
    "    returns an integer list of predicted topic \n",
    "    categories for a given topic matrix\n",
    "    '''\n",
    "    keys = topic_matrix.argmax(axis=1).tolist()\n",
    "    return keys\n",
    "\n",
    "def keys_to_counts(keys):\n",
    "    '''\n",
    "    returns a tuple of topic categories and their \n",
    "    accompanying magnitudes for a given list of keys\n",
    "    '''\n",
    "    count_pairs = Counter(keys).items()\n",
    "    categories = [pair[0] for pair in count_pairs]\n",
    "    counts = [pair[1] for pair in count_pairs]\n",
    "    return (categories, counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_keys = get_keys(lsa_topic_matrix)\n",
    "lsa_categories, lsa_counts = keys_to_counts(lsa_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper functions\n",
    "def get_top_n_words_2(n, keys, document_term_matrix, count_vectorizer):\n",
    "    '''\n",
    "    returns a list of n_topic strings, where each string contains the n most common \n",
    "    words in a predicted category, in order\n",
    "    '''\n",
    "    top_word_indices = []\n",
    "    for topic in range(n_topics):\n",
    "        temp_vector_sum = 0\n",
    "        for i in range(len(keys)):\n",
    "            if keys[i] == topic:\n",
    "                temp_vector_sum += document_term_matrix[i]\n",
    "        temp_vector_sum = temp_vector_sum.toarray()\n",
    "        top_n_word_indices = np.flip(np.argsort(temp_vector_sum)[0][-n:],0)\n",
    "        top_word_indices.append(top_n_word_indices)   \n",
    "    top_words = []\n",
    "    for topic in top_word_indices:\n",
    "        topic_words = []\n",
    "        for index in topic:\n",
    "            temp_word_vector = np.zeros((1,document_term_matrix.shape[1]))\n",
    "            temp_word_vector[:,index] = 1\n",
    "            the_word = count_vectorizer.inverse_transform(temp_word_vector)[0][0]\n",
    "            topic_words.append(the_word.encode('ascii').decode('utf-8'))\n",
    "        top_words.append(\" \".join(topic_words))         \n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Printing out the topics using the most frequent words in each"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_words_lsa = get_top_n_words_2(10, lsa_keys, small_document_term_matrix, small_count_vectorizer)\n",
    "\n",
    "for i in range(len(top_n_words_lsa)):\n",
    "    print(\"Topic {}: \".format(i+1), top_n_words_lsa[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The relative magnitudes of each of these topics can then be easily visualised though use of a bar chart."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_3_words = get_top_n_words_2(3, lsa_keys, small_document_term_matrix, small_count_vectorizer)\n",
    "labels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in lsa_categories]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.bar(lsa_categories, lsa_counts);\n",
    "ax.set_xticks(lsa_categories);\n",
    "ax.set_xticklabels(labels);\n",
    "ax.set_ylabel('Number of headlines');\n",
    "ax.set_title('LSA topic counts');\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Modeling using TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your turn\n",
    "#Using the full data dataset\n",
    "small_count_vectorizer = TfidfVectorizer(stop_words='english', use_idf=True,max_features=40000)\n",
    "print('Headline before vectorization: {}'.format(reindexed_data[123]))\n",
    "small_document_term_matrix = small_count_vectorizer.fit_transform(reindexed_data)\n",
    "print('Headline after vectorization: \\n{}'.format(small_document_term_matrix[123]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defind number of topic\n",
    "n_topics = 8\n",
    "#Calling LSA model\n",
    "lsa_model = TruncatedSVD(n_components=n_topics)\n",
    "lsa_topic_matrix = lsa_model.fit_transform(small_document_term_matrix)\n",
    "lsa_keys = get_keys(lsa_topic_matrix)\n",
    "lsa_categories, lsa_counts = keys_to_counts(lsa_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Printing out topics\n",
    "top_n_words_lsa = get_top_n_words_2(10, lsa_keys, small_document_term_matrix, small_count_vectorizer)\n",
    "\n",
    "for i in range(len(top_n_words_lsa)):\n",
    "    print(\"Topic {}: \".format(i+1), top_n_words_lsa[i])\n",
    "\n",
    "###Plotting out topics\n",
    "top_3_words = get_top_n_words_2(3, lsa_keys, small_document_term_matrix, small_count_vectorizer)\n",
    "labels = ['Topic {}: \\n'.format(i) + top_3_words[i] for i in lsa_categories]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "ax.bar(lsa_categories, lsa_counts);\n",
    "ax.set_xticks(lsa_categories);\n",
    "ax.set_xticklabels(labels);\n",
    "ax.set_ylabel('Number of headlines');\n",
    "ax.set_title('LSA topic counts');\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
