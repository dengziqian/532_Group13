{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Question 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### In this activity, you don't need to rerun the code, since the CAE Jupyter system doesn't allow us to install the required package nltk. We have already run the code locally and are keeping the results below. You can finish this warm-up question 1 by just reading through the code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### If you are using your local Jupyter notebook, you can run the first cell to install the package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in d:\\python\\python\\lib\\site-packages (3.4.4)\n",
      "Requirement already satisfied: six in d:\\python\\python\\lib\\site-packages (from nltk) (1.12.0)\n"
     ]
    }
   ],
   "source": [
    "# Install a pip package in the current Jupyter kernel\n",
    "import sys\n",
    "!{sys.executable} -m pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex for removing punctuation\n",
    "import re\n",
    "# nltk: useful text processing library \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text:\n",
      "['fli', 'fli', 'fli']\n",
      "\n",
      "Lemmatized text:\n",
      "['fly', 'fly', 'fly']\n"
     ]
    }
   ],
   "source": [
    "## fly, flies, flying\n",
    "text = \"fly, flies, flying\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text) #removing punctuation\n",
    "tokenized = word_tokenize(cleaned) #breaking text into individual words\n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "stemmed = [stemmer.stem(token) for token in tokenized] #Stemming each word\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized] #Lemmatizating each word\n",
    "\n",
    "print(\"Stemmed text:\")\n",
    "print(stemmed)\n",
    "\n",
    "print(\"\\nLemmatized text:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text:\n",
      "['organ', 'organ', 'organ']\n",
      "\n",
      "Lemmatized text:\n",
      "['organize', 'organize', 'organize']\n"
     ]
    }
   ],
   "source": [
    "### organize, organizes, organizing\n",
    "text = \"organize, organizes, organizing\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text) #removing punctuation\n",
    "tokenized = word_tokenize(cleaned) #breaking text into individual words\n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "stemmed = [stemmer.stem(token) for token in tokenized] #Stemming each word\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized] #Lemmatizating each word\n",
    "\n",
    "print(\"Stemmed text:\")\n",
    "print(stemmed)\n",
    "\n",
    "print(\"\\nLemmatized text:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text:\n",
      "['univers', 'univers']\n",
      "\n",
      "Lemmatized text:\n",
      "['universe', 'university']\n"
     ]
    }
   ],
   "source": [
    "### universe, university\n",
    "text = \"universe, university\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text) #removing punctuation\n",
    "tokenized = word_tokenize(cleaned) #breaking text into individual words\n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "stemmed = [stemmer.stem(token) for token in tokenized] #Stemming each word\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized] #Lemmatizating each word\n",
    "\n",
    "print(\"Stemmed text:\")\n",
    "print(stemmed)\n",
    "\n",
    "print(\"\\nLemmatized text:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Did you find anything interesting? Did the meaning of the individual word change a lot after lemmatization and stemming?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warm-up Question 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This activity uses 3 three tweets from Donald Trump's Twiiter. They are tokenized beforehand since the required package is not installed.\n",
    "#### docA = \"I never said the pandemic was a Hoax! Who would say such a thing? I said that the Do Nothing Democrats, together with their Mainstream Media partners, are the Hoax. They have been called out & embarrassed on this, even admitting they were wrong, but continue to spread the lie!\"\n",
    "##### docB = \"The people that know me and know the history of our Country say that I am  the hardest working President in history. I don’t know about that, but I am a hard worker and have probably gotten more done in the first 3 1/2 years than any President in history. The Fake News hates it!\"\n",
    "##### docC = \"Does anybody get the meaning of what a so-called Noble (not Nobel) Prize is, especially as it pertains to Reporters and Journalists? Noble is defined as, “having or showing fine personal qualities or high moral principles and ideals.” Does sarcasm ever work?\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document A: ['I', 'never', 'said', 'the', 'pandemic', 'was', 'a', 'Hoax', 'Who', 'would', 'say', 'such', 'a', 'thing', 'I', 'said', 'that', 'the', 'Do', 'Nothing', 'Democrats', 'together', 'with', 'their', 'Mainstream', 'Media', 'partners', 'are', 'the', 'Hoax', 'They', 'have', 'been', 'called', 'out', 'embarrassed', 'on', 'this', 'even', 'admitting', 'they', 'were', 'wrong', 'but', 'continue', 'to', 'spread', 'the', 'lie']\n",
      "\n",
      "Document B: ['The', 'people', 'that', 'know', 'me', 'and', 'know', 'the', 'history', 'of', 'our', 'Country', 'say', 'that', 'I', 'am', 'the', 'hardest', 'working', 'President', 'in', 'history', 'I', 'don', 't', 'know', 'about', 'that', 'but', 'I', 'am', 'a', 'hard', 'worker', 'and', 'have', 'probably', 'gotten', 'more', 'done', 'in', 'the', 'first', '3', '1', '2', 'years', 'than', 'any', 'President', 'in', 'history', 'The', 'Fake', 'News', 'hates', 'it']\n",
      "\n",
      "Document C: ['Does', 'anybody', 'get', 'the', 'meaning', 'of', 'what', 'a', 'so', 'called', 'Noble', 'not', 'Nobel', 'Prize', 'is', 'especially', 'as', 'it', 'pertains', 'to', 'Reporters', 'and', 'Journalists', 'Noble', 'is', 'defined', 'as', 'having', 'or', 'showing', 'fine', 'personal', 'qualities', 'or', 'high', 'moral', 'principles', 'and', 'ideals', 'Does', 'sarcasm', 'ever', 'work']\n"
     ]
    }
   ],
   "source": [
    "bowA = ['I', 'never', 'said', 'the', 'pandemic', 'was', 'a', 'Hoax', 'Who', 'would', 'say', 'such', 'a', 'thing', 'I', 'said', 'that', 'the', 'Do', 'Nothing', 'Democrats', 'together', 'with', 'their', 'Mainstream', 'Media', 'partners', 'are', 'the', 'Hoax', 'They', 'have', 'been', 'called', 'out', 'embarrassed', 'on', 'this', 'even', 'admitting', 'they', 'were', 'wrong', 'but', 'continue', 'to', 'spread', 'the', 'lie']\n",
    "bowB = ['The', 'people', 'that', 'know', 'me', 'and', 'know', 'the', 'history', 'of', 'our', 'Country', 'say', 'that', 'I', 'am', 'the', 'hardest', 'working', 'President', 'in', 'history', 'I', 'don', 't', 'know', 'about', 'that', 'but', 'I', 'am', 'a', 'hard', 'worker', 'and', 'have', 'probably', 'gotten', 'more', 'done', 'in', 'the', 'first', '3', '1', '2', 'years', 'than', 'any', 'President', 'in', 'history', 'The', 'Fake', 'News', 'hates', 'it']\n",
    "bowC = ['Does', 'anybody', 'get', 'the', 'meaning', 'of', 'what', 'a', 'so', 'called', 'Noble', 'not', 'Nobel', 'Prize', 'is', 'especially', 'as', 'it', 'pertains', 'to', 'Reporters', 'and', 'Journalists', 'Noble', 'is', 'defined', 'as', 'having', 'or', 'showing', 'fine', 'personal', 'qualities', 'or', 'high', 'moral', 'principles', 'and', 'ideals', 'Does', 'sarcasm', 'ever', 'work']\n",
    "#bow stands for bag of words\n",
    "print(\"Document A: \"+str(bowA))\n",
    "print(\"\\nDocument B: \"+str(bowB))\n",
    "print(\"\\nDocument C: \"+str(bowC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1) Run the following code to construct a term-frequency matrix in a simple way\n",
    "#First create a word set that contains all words in all documents\n",
    "wordSet = set(bowA).union(set(bowB)).union(set(bowC))\n",
    "#Second create dictionaries to keep my word counts.\n",
    "wordDictA = dict.fromkeys(wordSet,0)\n",
    "wordDictB = dict.fromkeys(wordSet,0)\n",
    "wordDictC = dict.fromkeys(wordSet,0)\n",
    "#Then count the words in each document\n",
    "for word in bowA:\n",
    "    wordDictA[word] += 1\n",
    "for word in bowB:\n",
    "    wordDictB[word] += 1\n",
    "for word in bowC:\n",
    "    wordDictC[word] += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>Country</th>\n",
       "      <th>Democrats</th>\n",
       "      <th>Do</th>\n",
       "      <th>Does</th>\n",
       "      <th>Fake</th>\n",
       "      <th>Hoax</th>\n",
       "      <th>I</th>\n",
       "      <th>...</th>\n",
       "      <th>was</th>\n",
       "      <th>were</th>\n",
       "      <th>what</th>\n",
       "      <th>with</th>\n",
       "      <th>work</th>\n",
       "      <th>worker</th>\n",
       "      <th>working</th>\n",
       "      <th>would</th>\n",
       "      <th>wrong</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2  3  Country  Democrats  Do  Does  Fake  Hoax  I  ...  was  were  what  \\\n",
       "0  0  0  0        0          1   1     0     0     2  2  ...    1     1     0   \n",
       "1  1  1  1        1          0   0     0     1     0  3  ...    0     0     0   \n",
       "2  0  0  0        0          0   0     2     0     0  0  ...    0     0     1   \n",
       "\n",
       "   with  work  worker  working  would  wrong  years  \n",
       "0     1     0       0        0      1      1      0  \n",
       "1     0     0       1        1      0      0      1  \n",
       "2     0     1       0        0      0      0      0  \n",
       "\n",
       "[3 rows x 106 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame([wordDictA, wordDictB, wordDictC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2) Complete the following code to construct the matrix using TF-IDF\n",
    "#First define a function to compute the term frequency TF\n",
    "def computeTF(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bowCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfBowA = computeTF(wordDictA, bowA)\n",
    "tfBowB = computeTF(wordDictB, bowB)\n",
    "tfBowC = computeTF(wordDictC, bowC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cell below has an unfinished equation that needs to be completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second define a function to compute the inverse document frequency IDF\n",
    "def computeIDF(docList):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    #counts the number of documents that contain a word w\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    #divide N by denominator above, take the log of that\n",
    "    for word, val in idfDict.items():\n",
    "        #Complete the code here\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF([wordDictA, wordDictB, wordDictC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lastly, compute TF-IDF\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>Country</th>\n",
       "      <th>Democrats</th>\n",
       "      <th>Do</th>\n",
       "      <th>Does</th>\n",
       "      <th>Fake</th>\n",
       "      <th>Hoax</th>\n",
       "      <th>I</th>\n",
       "      <th>...</th>\n",
       "      <th>was</th>\n",
       "      <th>were</th>\n",
       "      <th>what</th>\n",
       "      <th>with</th>\n",
       "      <th>work</th>\n",
       "      <th>worker</th>\n",
       "      <th>working</th>\n",
       "      <th>would</th>\n",
       "      <th>wrong</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022421</td>\n",
       "      <td>0.022421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044841</td>\n",
       "      <td>0.01655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022421</td>\n",
       "      <td>0.022421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022421</td>\n",
       "      <td>0.022421</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019274</td>\n",
       "      <td>0.019274</td>\n",
       "      <td>0.019274</td>\n",
       "      <td>0.019274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.02134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019274</td>\n",
       "      <td>0.019274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3   Country  Democrats        Do      Does  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000   0.022421  0.022421  0.000000   \n",
       "1  0.019274  0.019274  0.019274  0.019274   0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.051098   \n",
       "\n",
       "       Fake      Hoax        I  ...       was      were      what      with  \\\n",
       "0  0.000000  0.044841  0.01655  ...  0.022421  0.022421  0.000000  0.022421   \n",
       "1  0.019274  0.000000  0.02134  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.00000  ...  0.000000  0.000000  0.025549  0.000000   \n",
       "\n",
       "       work    worker   working     would     wrong     years  \n",
       "0  0.000000  0.000000  0.000000  0.022421  0.022421  0.000000  \n",
       "1  0.000000  0.019274  0.019274  0.000000  0.000000  0.019274  \n",
       "2  0.025549  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[3 rows x 106 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfBowA = computeTFIDF(tfBowA, idfs)\n",
    "tfidfBowB = computeTFIDF(tfBowB, idfs)\n",
    "tfidfBowC = computeTFIDF(tfBowC, idfs)\n",
    "pd.DataFrame([tfidfBowA, tfidfBowB, tfidfBowC])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm-up Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this question, we will use a dataset that contains posts from a data science forum\n",
    "postDocs = pd.read_csv(\"post_parsed.csv\")\n",
    "postDocs = postDocs[\"text\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create own list of stopwords\n",
    "nltk_stopword = [\"i\", \"me\", \"my\", \"myself\", \"we\", \"our\", \"ours\", \"ourselves\", \"you\", \"your\", \"yours\", \"yourself\", \"yourselves\", \"he\", \"him\", \"his\", \"himself\", \"she\", \"her\", \"hers\", \"herself\", \"it\", \"its\", \"itself\", \"they\", \"them\", \"their\", \"theirs\", \"themselves\", \"what\", \"which\", \"who\", \"whom\", \"this\", \"that\", \"these\", \"those\", \"am\", \"is\", \"are\", \"was\", \"were\", \"be\", \"been\", \"being\", \"have\", \"has\", \"had\", \"having\", \"do\", \"does\", \"did\", \"doing\", \"a\", \"an\", \"the\", \"and\", \"but\", \"if\", \"or\", \"because\", \"as\", \"until\", \"while\", \"of\", \"at\", \"by\", \"for\", \"with\", \"about\", \"against\", \"between\", \"into\", \"through\", \"during\", \"before\", \"after\", \"above\", \"below\", \"to\", \"from\", \"up\", \"down\", \"in\", \"out\", \"on\", \"off\", \"over\", \"under\", \"again\", \"further\", \"then\", \"once\", \"here\", \"there\", \"when\", \"where\", \"why\", \"how\", \"all\", \"any\", \"both\", \"each\", \"few\", \"more\", \"most\", \"other\", \"some\", \"such\", \"no\", \"nor\", \"not\", \"only\", \"own\", \"same\", \"so\", \"than\", \"too\", \"very\", \"s\", \"t\", \"can\", \"will\", \"just\", \"don\", \"should\", \"now\"]\n",
    "stopset = set(nltk_stopword)\n",
    "stopset.update(['lt','p','/p','br','amp','quot','field','font','normal','span','0px','rgb','style','51', \n",
    "                'spacing','text','helvetica','size','family', 'space', 'arial', 'height', 'indent', 'letter'\n",
    "                'line','none','sans','serif','transform','line','variant','weight','times', 'new','strong', 'video', 'title'\n",
    "                'white','word','letter', 'roman','0pt','16','color','12','14','21', 'neue', 'apple', 'class',  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>data science is about analyzing relevant data to obtain patterns of information in order to help achieve a goal. the main focus of the data analysis is the goal rather then the methodology on how it will achieved. this allows for creative thinking and allowing for the optimal solution or model to be found wihtout the constraint of a specific methodology.</p>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use scikit-learn's TF-IDF vectorizer to take my corpus and convert each document into a sparse matrix of TFIDF Features\n",
    "#Before vectorizing\n",
    "postDocs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<27x3451 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4091 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopset, use_idf=True, ngram_range=(1, 3)) #ngram_range allows the vectorizer to tokenize not only a single word but also bigram and trigram\n",
    "X = vectorizer.fit_transform(postDocs)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorized X is a sparse matrix with lots of 0 elements, which means there are lots of TF-IDF scores equal to 0.To save some memory of the machine, the matrix only stores the elements that have a score and washes out the 0 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3451 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 89 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 654)\t0.08989762673289421\n",
      "  (0, 2484)\t0.031055663877907407\n",
      "  (0, 160)\t0.06750601481982778\n",
      "  (0, 2413)\t0.0882767769894731\n",
      "  (0, 2042)\t0.10904753915911843\n",
      "  (0, 2156)\t0.09689742217847831\n",
      "  (0, 1588)\t0.052957348020145734\n",
      "  (0, 2087)\t0.07612666000883297\n",
      "  (0, 1472)\t0.07612666000883297\n",
      "  (0, 47)\t0.09689742217847831\n",
      "  (0, 1389)\t0.1765535539789462\n",
      "  (0, 1817)\t0.10904753915911843\n",
      "  (0, 1261)\t0.0882767769894731\n",
      "  (0, 143)\t0.07150739991692207\n",
      "  (0, 2378)\t0.10904753915911843\n",
      "  (0, 1918)\t0.21809507831823685\n",
      "  (0, 52)\t0.10904753915911843\n",
      "  (0, 108)\t0.10904753915911843\n",
      "  (0, 630)\t0.09689742217847831\n",
      "  (0, 2980)\t0.09689742217847831\n",
      "  (0, 105)\t0.10904753915911843\n",
      "  (0, 2081)\t0.10904753915911843\n",
      "  (0, 2755)\t0.09689742217847831\n",
      "  (0, 1946)\t0.07612666000883297\n",
      "  (0, 1295)\t0.0882767769894731\n",
      "  :\t:\n",
      "  (0, 2044)\t0.10904753915911843\n",
      "  (0, 2158)\t0.10904753915911843\n",
      "  (0, 1600)\t0.10904753915911843\n",
      "  (0, 2093)\t0.10904753915911843\n",
      "  (0, 1474)\t0.10904753915911843\n",
      "  (0, 49)\t0.10904753915911843\n",
      "  (0, 1391)\t0.10904753915911843\n",
      "  (0, 1819)\t0.10904753915911843\n",
      "  (0, 1265)\t0.10904753915911843\n",
      "  (0, 662)\t0.10904753915911843\n",
      "  (0, 147)\t0.10904753915911843\n",
      "  (0, 1395)\t0.10904753915911843\n",
      "  (0, 2380)\t0.10904753915911843\n",
      "  (0, 1920)\t0.10904753915911843\n",
      "  (0, 54)\t0.10904753915911843\n",
      "  (0, 110)\t0.10904753915911843\n",
      "  (0, 632)\t0.10904753915911843\n",
      "  (0, 2982)\t0.10904753915911843\n",
      "  (0, 107)\t0.10904753915911843\n",
      "  (0, 2083)\t0.10904753915911843\n",
      "  (0, 2759)\t0.10904753915911843\n",
      "  (0, 1954)\t0.10904753915911843\n",
      "  (0, 1301)\t0.10904753915911843\n",
      "  (0, 3327)\t0.10904753915911843\n",
      "  (0, 566)\t0.10904753915911843\n"
     ]
    }
   ],
   "source": [
    "#After vectorizing\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though 3451 elements are in the original matrix, the vectorizer only scores 89 of them. For example, the 654th word in the first document has a TF-IDF score of 0.08989762673289421 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 3451)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Latent Semantic Analysis\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=27, n_iter=100,\n",
       "             random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa = TruncatedSVD(n_components=27, n_iter=100) # n_components specifies the number of topics.\n",
    "lsa.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA\n",
    "\n",
    "$$X \\approx USV^{T}$$\n",
    "\n",
    "Input: X, a matrix where m is the number of documents, and n is the number of terms.\n",
    "\n",
    "U will be a m x k matrix. The rows will be documents and the columns will be 'topics'\n",
    "\n",
    "S will be a k x k diagnal matrix. The elements will be the amount of variation captured from each topic.\n",
    "\n",
    "V will be a n x k matrix. The rows will be terms and the columns will be topics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00476404, 0.00476404, 0.00476404, ..., 0.00476404, 0.00476404,\n",
       "       0.00476404])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the first row for V\n",
    "lsa.components_[0] #lsa.components_ is designed to represent V, which is the term by topic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "data\n",
      "large amounts\n",
      "large amounts data\n",
      "amounts\n",
      "amounts data\n",
      "different\n",
      "large\n",
      "science\n",
      "used\n",
      "data science\n",
      " \n",
      "Topic 1:\n",
      "large amounts\n",
      "large amounts data\n",
      "amounts\n",
      "amounts data\n",
      "used\n",
      "large\n",
      "according data\n",
      "according data science\n",
      "amounts data procedures聽could\n",
      "amounts data since\n",
      " \n",
      "Topic 2:\n",
      "white\n",
      "converted\n",
      "white converted\n",
      "white white\n",
      "big\n",
      "big data\n",
      "hello\n",
      "and聽 white\n",
      "and聽 white converted\n",
      "big data and聽\n",
      " \n",
      "Topic 3:\n",
      "make\n",
      "decisions\n",
      "problem\n",
      "make better\n",
      "data science analyzing\n",
      "science analyzing\n",
      "white\n",
      "better\n",
      "better decisions\n",
      "make better decisions\n",
      " \n",
      "Topic 4:\n",
      "goal\n",
      "data science analyzing\n",
      "science analyzing\n",
      "achieve\n",
      "solution\n",
      "methodology\n",
      "relevant\n",
      "relevant data\n",
      "information\n",
      "finding\n",
      " \n",
      "Topic 5:\n",
      "business\n",
      "goal\n",
      "methods\n",
      "competitive edge\n",
      "edge\n",
      "especially\n",
      "achieve\n",
      "solution\n",
      "perspective\n",
      "analyzing\n",
      " \n",
      "Topic 6:\n",
      "art\n",
      "answer\n",
      "using\n",
      "relevant\n",
      "relevant data\n",
      "part\n",
      "using relevant\n",
      "using relevant data\n",
      "good\n",
      "able learn\n",
      " \n",
      "Topic 7:\n",
      "data science\n",
      "science\n",
      "ultimately\n",
      "understanding\n",
      "predict\n",
      "insights\n",
      "people\n",
      "information\n",
      "digital\n",
      "trends\n",
      " \n",
      "Topic 8:\n",
      "questions\n",
      "order\n",
      "answer\n",
      "statistics\n",
      "gaining\n",
      "research\n",
      "technologies\n",
      "complex\n",
      "everyone\n",
      "hello everyone\n",
      " \n",
      "Topic 9:\n",
      "users\n",
      "find\n",
      "far\n",
      "learn\n",
      "dig\n",
      "building\n",
      "methods\n",
      "actionable\n",
      "actions\n",
      "data doesn\n",
      " \n",
      "Topic 10:\n",
      "predict\n",
      "ability\n",
      "videos\n",
      "ability explain understand\n",
      "accessing\n",
      "explain\n",
      "explain understand\n",
      "explain understand predict\n",
      "information organized\n",
      "information organized significant\n",
      " \n",
      "Topic 11:\n",
      "applying\n",
      "applying making\n",
      "applying making use\n",
      "belive\n",
      "belive data\n",
      "belive data science\n",
      "building efficient\n",
      "building efficient model\n",
      "course\n",
      "course thanks聽\n",
      " \n",
      "Topic 12:\n",
      "ultimately\n",
      "questions\n",
      "users\n",
      "get\n",
      "greater\n",
      "humanity\n",
      "actionable\n",
      "actions\n",
      "data doesn\n",
      "farming\n",
      " \n",
      "Topic 13:\n",
      "dig\n",
      "decisions\n",
      "archaeology\n",
      "history\n",
      "questions\n",
      "computer\n",
      "organization\n",
      "find\n",
      "gaining\n",
      "problem\n",
      " \n",
      "Topic 14:\n",
      "digital\n",
      "trends\n",
      "perspective\n",
      "many\n",
      "huge\n",
      "methods\n",
      "involves\n",
      "get\n",
      "age\n",
      "age information\n",
      " \n",
      "Topic 15:\n",
      "people\n",
      "greater\n",
      "humanity\n",
      "problems\n",
      "find\n",
      "ultimately\n",
      "one\n",
      "solve\n",
      "hadoop\n",
      "technical\n",
      " \n",
      "Topic 16:\n",
      "design\n",
      "part\n",
      "answer\n",
      "substantive\n",
      "skills\n",
      "child\n",
      "collect\n",
      "lego\n",
      "art\n",
      "cleaning\n",
      " \n",
      "Topic 17:\n",
      "ability\n",
      "part\n",
      "answer\n",
      "significant\n",
      "gained\n",
      "dig\n",
      "ultimately\n",
      "ability explain\n",
      "accessing\n",
      "explain\n",
      " \n",
      "Topic 18:\n",
      "statistics data\n",
      "studies\n",
      "use statistics\n",
      "use statistics data\n",
      "greater\n",
      "humanity\n",
      "fields\n",
      "use\n",
      "research\n",
      "easily\n",
      " \n",
      "Topic 19:\n",
      "perspective\n",
      "many\n",
      "use data\n",
      "methods\n",
      "form\n",
      "others\n",
      "technologies\n",
      "used\n",
      "finding data\n",
      "finding\n",
      " \n",
      "Topic 20:\n",
      "order\n",
      "methodology\n",
      "answers\n",
      "model\n",
      "information\n",
      "good\n",
      "answers to聽questions\n",
      "answers to聽questions using\n",
      "appropriate\n",
      "appropriate scientific\n",
      " \n",
      "Topic 21:\n",
      "use data\n",
      "technologies\n",
      "amounts data help\n",
      "analyzing data especially\n",
      "bi\n",
      "bi experts\n",
      "bi experts excels\n",
      "business intelligent\n",
      "business intelligent bi\n",
      "business pick\n",
      " \n",
      "Topic 22:\n",
      "achieve ultimate\n",
      "achieve ultimate goal\n",
      "analyzing existing\n",
      "analyzing existing data\n",
      "creativity\n",
      "creativity achieve\n",
      "creativity achieve ultimate\n",
      "data information\n",
      "data information finding\n",
      "data science figure\n",
      " \n",
      "Topic 23:\n",
      "problem\n",
      "finding\n",
      "intelligent\n",
      "answers\n",
      "akhilesh\n",
      "analyzing problem\n",
      "analyzing problem finding\n",
      "better decisions creative\n",
      "creative thinking thank\n",
      "data intensive\n",
      " \n",
      "Topic 24:\n",
      "canada\n",
      "contacts\n",
      "questions\n",
      "would\n",
      "asked\n",
      "contacts canada\n",
      "even\n",
      "really\n",
      "using data\n",
      "want\n",
      " \n",
      "Topic 25:\n",
      "20 white\n",
      "47\n",
      "armed\n",
      "armed practical\n",
      "armed practical experience\n",
      "ata\n",
      "ata science\n",
      "ata science 聽is\n",
      "basics\n",
      "basics data\n",
      " \n",
      "Topic 26:\n",
      "data\n",
      "30\n",
      "ability explain understand\n",
      "could\n",
      "able design simple\n",
      "large\n",
      "come\n",
      "able make\n",
      "amounts\n",
      "amounts data\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Attach the terms to the components and print the first 10 terms that make up the topics\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_): \n",
    "    termsInComp = zip (terms,comp)\n",
    "    sortedTerms =  sorted(termsInComp, key=lambda x: x[1], reverse=True) [:10]\n",
    "    print(\"Topic %d:\" % i )\n",
    "    for term in sortedTerms:\n",
    "        print(term[0])\n",
    "    print (\" \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After printing out the topics and their related terms, could you summarize the topics using your own words?   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try to use a sentence or a phrase to summarize topic 3, 4 and 7. For example, topic 0 could be summed up as “data science uses big data”."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
