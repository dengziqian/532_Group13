{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm-up Question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regex for removing punctuation\n",
    "import re\n",
    "# nltk: useful text processing library \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "def get_part_of_speech(word):\n",
    "  probable_part_of_speech = wordnet.synsets(word)\n",
    "  pos_counts = Counter()\n",
    "  pos_counts[\"n\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"n\"]  )\n",
    "  pos_counts[\"v\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"v\"]  )\n",
    "  pos_counts[\"a\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"a\"]  )\n",
    "  pos_counts[\"r\"] = len(  [ item for item in probable_part_of_speech if item.pos()==\"r\"]  )\n",
    "  \n",
    "  most_likely_part_of_speech = pos_counts.most_common(1)[0][0]\n",
    "  return most_likely_part_of_speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\45098\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"feet, foot, foots, footing\"\n",
    "\n",
    "cleaned = re.sub('\\W+', ' ', text) #removing punctuation\n",
    "tokenized = word_tokenize(cleaned) #breaking text into individual words\n",
    "\n",
    "stemmer = PorterStemmer() \n",
    "stemmed = [stemmer.stem(token) for token in tokenized] #Stemming each word\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized = [lemmatizer.lemmatize(token, get_part_of_speech(token)) for token in tokenized] #Lemmatizating each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed text:\n",
      "['feet', 'foot', 'foot', 'foot']\n",
      "\n",
      "Lemmatized text:\n",
      "['foot', 'foot', 'foot', 'footing']\n"
     ]
    }
   ],
   "source": [
    "print(\"Stemmed text:\")\n",
    "print(stemmed)\n",
    "\n",
    "print(\"\\nLemmatized text:\")\n",
    "print(lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm-up Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document A: ['I', 'never', 'said', 'the', 'pandemic', 'was', 'a', 'Hoax', 'Who', 'would', 'say', 'such', 'a', 'thing', 'I', 'said', 'that', 'the', 'Do', 'Nothing', 'Democrats', 'together', 'with', 'their', 'Mainstream', 'Media', 'partners', 'are', 'the', 'Hoax', 'They', 'have', 'been', 'called', 'out', 'embarrassed', 'on', 'this', 'even', 'admitting', 'they', 'were', 'wrong', 'but', 'continue', 'to', 'spread', 'the', 'lie']\n",
      "Document B: ['The', 'people', 'that', 'know', 'me', 'and', 'know', 'the', 'history', 'of', 'our', 'Country', 'say', 'that', 'I', 'am', 'the', 'hardest', 'working', 'President', 'in', 'history', 'I', 'don', 't', 'know', 'about', 'that', 'but', 'I', 'am', 'a', 'hard', 'worker', 'and', 'have', 'probably', 'gotten', 'more', 'done', 'in', 'the', 'first', '3', '1', '2', 'years', 'than', 'any', 'President', 'in', 'history', 'The', 'Fake', 'News', 'hates', 'it']\n",
      "Document C: ['Does', 'anybody', 'get', 'the', 'meaning', 'of', 'what', 'a', 'so', 'called', 'Noble', 'not', 'Nobel', 'Prize', 'is', 'especially', 'as', 'it', 'pertains', 'to', 'Reporters', 'and', 'Journalists', 'Noble', 'is', 'defined', 'as', 'having', 'or', 'showing', 'fine', 'personal', 'qualities', 'or', 'high', 'moral', 'principles', 'and', 'ideals', 'Does', 'sarcasm', 'ever', 'work']\n"
     ]
    }
   ],
   "source": [
    "docA = \"I never said the pandemic was a Hoax! Who would say such a thing? I said that the Do Nothing Democrats, together with their Mainstream Media partners, are the Hoax. They have been called out & embarrassed on this, even admitting they were wrong, but continue to spread the lie!\"\n",
    "docB = \"The people that know me and know the history of our Country say that I am  the hardest working President in history. I don’t know about that, but I am a hard worker and have probably gotten more done in the first 3 1/2 years than any President in history. The Fake News hates it!\"\n",
    "docC = \"Does anybody get the meaning of what a so-called Noble (not Nobel) Prize is, especially as it pertains to Reporters and Journalists? Noble is defined as, “having or showing fine personal qualities or high moral principles and ideals.” Does sarcasm ever work?\"\n",
    "bowA = word_tokenize(re.sub('\\W+', ' ', docA)) #bow stands for bag of words\n",
    "bowB = word_tokenize(re.sub('\\W+', ' ', docB))\n",
    "bowC = word_tokenize(re.sub('\\W+', ' ', docC))\n",
    "print(\"Document A: \"+str(bowA))\n",
    "print(\"Document B: \"+str(bowB))\n",
    "print(\"Document C: \"+str(bowC))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 1) Run the following code to construct a term-frequency matrix in a simple way\n",
    "#First create a word set that contains all words in all documents\n",
    "wordSet = set(bowA).union(set(bowB)).union(set(bowC))\n",
    "#Second create dictionaries to keep my word counts.\n",
    "wordDictA = dict.fromkeys(wordSet,0)\n",
    "wordDictB = dict.fromkeys(wordSet,0)\n",
    "wordDictC = dict.fromkeys(wordSet,0)\n",
    "#Then count the words in each document\n",
    "for word in bowA:\n",
    "    wordDictA[word] += 1\n",
    "for word in bowB:\n",
    "    wordDictB[word] += 1\n",
    "for word in bowC:\n",
    "    wordDictC[word] += 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>Country</th>\n",
       "      <th>Democrats</th>\n",
       "      <th>Do</th>\n",
       "      <th>Does</th>\n",
       "      <th>Fake</th>\n",
       "      <th>Hoax</th>\n",
       "      <th>I</th>\n",
       "      <th>...</th>\n",
       "      <th>was</th>\n",
       "      <th>were</th>\n",
       "      <th>what</th>\n",
       "      <th>with</th>\n",
       "      <th>work</th>\n",
       "      <th>worker</th>\n",
       "      <th>working</th>\n",
       "      <th>would</th>\n",
       "      <th>wrong</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   1  2  3  Country  Democrats  Do  Does  Fake  Hoax  I  ...  was  were  what  \\\n",
       "0  0  0  0        0          1   1     0     0     2  2  ...    1     1     0   \n",
       "1  1  1  1        1          0   0     0     1     0  3  ...    0     0     0   \n",
       "2  0  0  0        0          0   0     2     0     0  0  ...    0     0     1   \n",
       "\n",
       "   with  work  worker  working  would  wrong  years  \n",
       "0     1     0       0        0      1      1      0  \n",
       "1     0     0       1        1      0      0      1  \n",
       "2     0     1       0        0      0      0      0  \n",
       "\n",
       "[3 rows x 106 columns]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.DataFrame([wordDictA, wordDictB, wordDictC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 2) Complete the following code to construct the matrix using TF-IDF\n",
    "#First define a function to compute the term frequency TF\n",
    "def computeTF(wordDict, bow):\n",
    "    tfDict = {}\n",
    "    bowCount = len(bow)\n",
    "    for word, count in wordDict.items():\n",
    "        tfDict[word] = count / float(bowCount)\n",
    "    return tfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfBowA = computeTF(wordDictA, bowA)\n",
    "tfBowB = computeTF(wordDictB, bowB)\n",
    "tfBowC = computeTF(wordDictC, bowC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Second define a function to compute the inverse document frequency IDF\n",
    "def computeIDF(docList):\n",
    "    import math\n",
    "    idfDict = {}\n",
    "    N = len(docList)\n",
    "    \n",
    "    #counts the number of documents that contain a word w\n",
    "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
    "    for doc in docList:\n",
    "        for word, val in doc.items():\n",
    "            if val > 0:\n",
    "                idfDict[word] += 1\n",
    "    \n",
    "    #divide N by denominator above, take the log of that\n",
    "    for word, val in idfDict.items():\n",
    "        #Complete the code here\n",
    "        idfDict[word] = math.log(N / float(val))\n",
    "    \n",
    "    return idfDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "idfs = computeIDF([wordDictA, wordDictB, wordDictC])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lastly, compute TF-IDF\n",
    "def computeTFIDF(tfBow, idfs):\n",
    "    tfidf = {}\n",
    "    for word, val in tfBow.items():\n",
    "        tfidf[word] = val * idfs[word]\n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>Country</th>\n",
       "      <th>Democrats</th>\n",
       "      <th>Do</th>\n",
       "      <th>Does</th>\n",
       "      <th>Fake</th>\n",
       "      <th>Hoax</th>\n",
       "      <th>I</th>\n",
       "      <th>...</th>\n",
       "      <th>was</th>\n",
       "      <th>were</th>\n",
       "      <th>what</th>\n",
       "      <th>with</th>\n",
       "      <th>work</th>\n",
       "      <th>worker</th>\n",
       "      <th>working</th>\n",
       "      <th>would</th>\n",
       "      <th>wrong</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022421</td>\n",
       "      <td>0.022421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.044841</td>\n",
       "      <td>0.01655</td>\n",
       "      <td>...</td>\n",
       "      <td>0.022421</td>\n",
       "      <td>0.022421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022421</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022421</td>\n",
       "      <td>0.022421</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.019274</td>\n",
       "      <td>0.019274</td>\n",
       "      <td>0.019274</td>\n",
       "      <td>0.019274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.02134</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019274</td>\n",
       "      <td>0.019274</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.051098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.025549</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 106 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          1         2         3   Country  Democrats        Do      Does  \\\n",
       "0  0.000000  0.000000  0.000000  0.000000   0.022421  0.022421  0.000000   \n",
       "1  0.019274  0.019274  0.019274  0.019274   0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.000000  0.000000   0.000000  0.000000  0.051098   \n",
       "\n",
       "       Fake      Hoax        I  ...       was      were      what      with  \\\n",
       "0  0.000000  0.044841  0.01655  ...  0.022421  0.022421  0.000000  0.022421   \n",
       "1  0.019274  0.000000  0.02134  ...  0.000000  0.000000  0.000000  0.000000   \n",
       "2  0.000000  0.000000  0.00000  ...  0.000000  0.000000  0.025549  0.000000   \n",
       "\n",
       "       work    worker   working     would     wrong     years  \n",
       "0  0.000000  0.000000  0.000000  0.022421  0.022421  0.000000  \n",
       "1  0.000000  0.019274  0.019274  0.000000  0.000000  0.019274  \n",
       "2  0.025549  0.000000  0.000000  0.000000  0.000000  0.000000  \n",
       "\n",
       "[3 rows x 106 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidfBowA = computeTFIDF(tfBowA, idfs)\n",
    "tfidfBowB = computeTFIDF(tfBowB, idfs)\n",
    "tfidfBowC = computeTFIDF(tfBowC, idfs)\n",
    "pd.DataFrame([tfidfBowA, tfidfBowB, tfidfBowC])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Warm-up Question 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In this question, we will use a dataset that contains posts from a data science forum\n",
    "#Change the directory path here\n",
    "posts = open('.../raw_forum_posts.dat', 'r').read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data preprocessing\n",
    "soup = BeautifulSoup(posts, 'lxml') #The package BeautifulSoup is here used to parse the XML data\n",
    "postTxt = soup.findAll('text')  #all posts <text> \n",
    "postDocs = [x.text for x in postTxt]\n",
    "postDocs.pop(0)\n",
    "postDocs = [x.lower() for x in postDocs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create own list of stopwords\n",
    "stopset = set(stopwords.words('english'))\n",
    "stopset.update(['lt','p','/p','br','amp','quot','field','font','normal','span','0px','rgb','style','51', \n",
    "                'spacing','text','helvetica','size','family', 'space', 'arial', 'height', 'indent', 'letter'\n",
    "                'line','none','sans','serif','transform','line','variant','weight','times', 'new','strong', 'video', 'title'\n",
    "                'white','word','letter', 'roman','0pt','16','color','12','14','21', 'neue', 'apple', 'class',  ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>data science is about analyzing relevant data to obtain patterns of information in order to help achieve a goal. the main focus of the data analysis is the goal rather then the methodology on how it will achieved. this allows for creative thinking and allowing for the optimal solution or model to be found wihtout the constraint of a specific methodology.</p>'"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use scikit-learn's TF-IDF vectorizer to take my corpus and convert each document into a sparse matrix of TFIDF Features\n",
    "#Before vectorizing\n",
    "postDocs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<27x3428 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 4064 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopset, use_idf=True, ngram_range=(1, 3)) #ngram_range allows the vectorizer to tokenize not only a single word but also bigram and trigram\n",
    "X = vectorizer.fit_transform(postDocs)\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The vectorized X is a sparse matrix with lots of 0 elements, which means there are lots of TF-IDF scores equal to 0.To save some memory of the machine, the matrix only stores the elements that have a score and washes out the 0 elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x3428 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 89 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 649)\t0.08989762673289421\n",
      "  (0, 2473)\t0.031055663877907407\n",
      "  (0, 160)\t0.06750601481982778\n",
      "  (0, 2402)\t0.0882767769894731\n",
      "  (0, 2031)\t0.10904753915911843\n",
      "  (0, 2145)\t0.09689742217847831\n",
      "  (0, 1577)\t0.052957348020145734\n",
      "  (0, 2076)\t0.07612666000883297\n",
      "  (0, 1461)\t0.07612666000883297\n",
      "  (0, 47)\t0.09689742217847831\n",
      "  (0, 1378)\t0.1765535539789462\n",
      "  (0, 1806)\t0.10904753915911843\n",
      "  (0, 1250)\t0.0882767769894731\n",
      "  (0, 143)\t0.07150739991692207\n",
      "  (0, 2367)\t0.10904753915911843\n",
      "  (0, 1907)\t0.21809507831823685\n",
      "  (0, 52)\t0.10904753915911843\n",
      "  (0, 108)\t0.10904753915911843\n",
      "  (0, 625)\t0.09689742217847831\n",
      "  (0, 2969)\t0.09689742217847831\n",
      "  (0, 105)\t0.10904753915911843\n",
      "  (0, 2070)\t0.10904753915911843\n",
      "  (0, 2744)\t0.09689742217847831\n",
      "  (0, 1935)\t0.07612666000883297\n",
      "  (0, 1284)\t0.0882767769894731\n",
      "  :\t:\n",
      "  (0, 2033)\t0.10904753915911843\n",
      "  (0, 2147)\t0.10904753915911843\n",
      "  (0, 1589)\t0.10904753915911843\n",
      "  (0, 2082)\t0.10904753915911843\n",
      "  (0, 1463)\t0.10904753915911843\n",
      "  (0, 49)\t0.10904753915911843\n",
      "  (0, 1380)\t0.10904753915911843\n",
      "  (0, 1808)\t0.10904753915911843\n",
      "  (0, 1254)\t0.10904753915911843\n",
      "  (0, 657)\t0.10904753915911843\n",
      "  (0, 147)\t0.10904753915911843\n",
      "  (0, 1384)\t0.10904753915911843\n",
      "  (0, 2369)\t0.10904753915911843\n",
      "  (0, 1909)\t0.10904753915911843\n",
      "  (0, 54)\t0.10904753915911843\n",
      "  (0, 110)\t0.10904753915911843\n",
      "  (0, 627)\t0.10904753915911843\n",
      "  (0, 2971)\t0.10904753915911843\n",
      "  (0, 107)\t0.10904753915911843\n",
      "  (0, 2072)\t0.10904753915911843\n",
      "  (0, 2748)\t0.10904753915911843\n",
      "  (0, 1943)\t0.10904753915911843\n",
      "  (0, 1290)\t0.10904753915911843\n",
      "  (0, 3304)\t0.10904753915911843\n",
      "  (0, 561)\t0.10904753915911843\n"
     ]
    }
   ],
   "source": [
    "#After vectorizing\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though 3428 elements are in the original matrix, the vectorizer only scores 89 of them. For example, the 649th word in the first document has a TF-IDF score of 0.08989762673289421 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27, 3428)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Latent Semantic Analysis\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=27, n_iter=100,\n",
       "             random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa = TruncatedSVD(n_components=27, n_iter=100) # n_components specifies the number of topics.\n",
    "lsa.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSA\n",
    "\n",
    "$$X \\approx USV^{T}$$\n",
    "\n",
    "Input: X, a matrix where m is the number of documents, and n is the number of terms.\n",
    "\n",
    "U will be a m x k matrix. The rows will be documents and the columns will be 'topics'\n",
    "\n",
    "S will be a k x k diagnal matrix. The elements will be the amount of variation captured from each topic.\n",
    "\n",
    "V will be a n x k matrix. The rows will be terms and the columns will be topics.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.00477467, 0.00477467, 0.00477467, ..., 0.00477467, 0.00477467,\n",
       "       0.00477467])"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#This is the first row for V\n",
    "lsa.components_[0] #lsa.components_ is designed to represent V, which is the term by topic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0:\n",
      "data\n",
      "large amounts\n",
      "large amounts data\n",
      "amounts\n",
      "amounts data\n",
      "different\n",
      "science\n",
      "large\n",
      "used\n",
      "data science\n",
      " \n",
      "Topic 1:\n",
      "large amounts\n",
      "large amounts data\n",
      "amounts\n",
      "amounts data\n",
      "used\n",
      "large\n",
      "procedures聽could plots mainly\n",
      "according data\n",
      "according data science\n",
      "amounts data procedures聽could\n",
      " \n",
      "Topic 2:\n",
      "white\n",
      "converted\n",
      "white converted\n",
      "white white\n",
      "big\n",
      "big data\n",
      "hello\n",
      "and聽 white\n",
      "and聽 white converted\n",
      "big data and聽\n",
      " \n",
      "Topic 3:\n",
      "make\n",
      "decisions\n",
      "problem\n",
      "make better\n",
      "data science analyzing\n",
      "science analyzing\n",
      "white\n",
      "better\n",
      "better decisions\n",
      "make better decisions\n",
      " \n",
      "Topic 4:\n",
      "goal\n",
      "data science analyzing\n",
      "science analyzing\n",
      "achieve\n",
      "solution\n",
      "methodology\n",
      "relevant\n",
      "relevant data\n",
      "information\n",
      "answer\n",
      " \n",
      "Topic 5:\n",
      "business\n",
      "goal\n",
      "methods\n",
      "competitive edge\n",
      "edge\n",
      "especially\n",
      "achieve\n",
      "solution\n",
      "perspective\n",
      "analyzing\n",
      " \n",
      "Topic 6:\n",
      "art\n",
      "answer\n",
      "using\n",
      "relevant\n",
      "relevant data\n",
      "using relevant\n",
      "using relevant data\n",
      "part\n",
      "good\n",
      "able learn data\n",
      " \n",
      "Topic 7:\n",
      "data scientist\n",
      "scientist\n",
      "questions\n",
      "answer\n",
      "competitive\n",
      "intelligent\n",
      "amounts\n",
      "amounts data\n",
      "canada\n",
      "contacts\n",
      " \n",
      "Topic 8:\n",
      "may\n",
      "predict\n",
      "future\n",
      "child\n",
      "collect\n",
      "lego\n",
      "data scientist\n",
      "provide\n",
      "ability\n",
      "people\n",
      " \n",
      "Topic 9:\n",
      "dig\n",
      "find\n",
      "far\n",
      "learn\n",
      "archaeology\n",
      "history\n",
      "finding\n",
      "define\n",
      "learn data\n",
      "learn data science\n",
      " \n",
      "Topic 10:\n",
      "predict\n",
      "ability\n",
      "videos\n",
      "way\n",
      "ability explain\n",
      "ability explain understand\n",
      "accessing\n",
      "explain\n",
      "explain understand\n",
      "explain understand predict\n",
      " \n",
      "Topic 11:\n",
      "set data\n",
      "applying\n",
      "applying making\n",
      "applying making use\n",
      "belive\n",
      "belive data\n",
      "belive data science\n",
      "building efficient\n",
      "building efficient model\n",
      "course\n",
      " \n",
      "Topic 12:\n",
      "ultimately\n",
      "users\n",
      "questions\n",
      "greater\n",
      "humanity\n",
      "actionable\n",
      "actions\n",
      "farming\n",
      "get lost\n",
      "lost\n",
      " \n",
      "Topic 13:\n",
      "dig\n",
      "decisions\n",
      "archaeology\n",
      "history\n",
      "computer\n",
      "organization\n",
      "questions\n",
      "gaining\n",
      "ways\n",
      "find\n",
      " \n",
      "Topic 14:\n",
      "digital\n",
      "trends\n",
      "perspective\n",
      "many\n",
      "methods\n",
      "involves\n",
      "huge\n",
      "age\n",
      "age information\n",
      "age information recorded\n",
      " \n",
      "Topic 15:\n",
      "people\n",
      "greater\n",
      "humanity\n",
      "ultimately\n",
      "problems\n",
      "solve\n",
      "find\n",
      "one\n",
      "techniques\n",
      "faster\n",
      " \n",
      "Topic 16:\n",
      "design\n",
      "child\n",
      "collect\n",
      "lego\n",
      "provide\n",
      "substantive\n",
      "available\n",
      "data available\n",
      "cleaning\n",
      "collect data\n",
      " \n",
      "Topic 17:\n",
      "part\n",
      "answer\n",
      "ability\n",
      "significant\n",
      "ultimately\n",
      "art\n",
      "part involves\n",
      "science part\n",
      "statistically\n",
      "accessing\n",
      " \n",
      "Topic 18:\n",
      "statistics data\n",
      "use statistics\n",
      "use statistics data\n",
      "studies\n",
      "greater\n",
      "humanity\n",
      "fields\n",
      "use\n",
      "research\n",
      "easily\n",
      " \n",
      "Topic 19:\n",
      "perspective\n",
      "many\n",
      "use data\n",
      "methods\n",
      "technologies\n",
      "form\n",
      "others\n",
      "used\n",
      "finding data\n",
      "finding\n",
      " \n",
      "Topic 20:\n",
      "methodology\n",
      "order\n",
      "answers\n",
      "model\n",
      "information\n",
      "good\n",
      "answers to聽questions\n",
      "answers to聽questions using\n",
      "appropriate\n",
      "appropriate scientific\n",
      " \n",
      "Topic 21:\n",
      "use data\n",
      "technologies\n",
      "amounts data help\n",
      "analyzing data especially\n",
      "bi\n",
      "bi experts\n",
      "bi experts excels\n",
      "business intelligent\n",
      "business intelligent bi\n",
      "business pick\n",
      " \n",
      "Topic 22:\n",
      "achieve ultimate\n",
      "achieve ultimate goal\n",
      "analyzing existing\n",
      "analyzing existing data\n",
      "creativity\n",
      "creativity achieve\n",
      "creativity achieve ultimate\n",
      "data information\n",
      "data information finding\n",
      "data science figure\n",
      " \n",
      "Topic 23:\n",
      "problem\n",
      "finding\n",
      "intelligent\n",
      "answers\n",
      "ability make intelligent\n",
      "ability make\n",
      "akhilesh\n",
      "analyzing problem\n",
      "analyzing problem finding\n",
      "better decisions creative\n",
      " \n",
      "Topic 24:\n",
      "questions\n",
      "canada\n",
      "contacts\n",
      "would\n",
      "asked\n",
      "contacts canada\n",
      "even\n",
      "really\n",
      "using data\n",
      "want\n",
      " \n",
      "Topic 25:\n",
      "20\n",
      "20 white ata\n",
      "47\n",
      "20 white\n",
      "armed\n",
      "armed practical\n",
      "armed practical experience\n",
      "ata\n",
      "ata science\n",
      "ata science 聽is\n",
      " \n",
      "Topic 26:\n",
      "20 white\n",
      "large amounts\n",
      "large amounts data\n",
      "tools\n",
      "practices\n",
      "large\n",
      "different\n",
      "ability predict pretty\n",
      "ability make intelligent\n",
      "able learn\n",
      " \n"
     ]
    }
   ],
   "source": [
    "#Attach the terms to the components and print the first 10 terms that make up the topics\n",
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_): \n",
    "    termsInComp = zip (terms,comp)\n",
    "    sortedTerms =  sorted(termsInComp, key=lambda x: x[1], reverse=True) [:10]\n",
    "    print(\"Topic %d:\" % i )\n",
    "    for term in sortedTerms:\n",
    "        print(term[0])\n",
    "    print (\" \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
